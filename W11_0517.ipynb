{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsiWkgh3dWkXaGuJTAX8FB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ynebin/class2024Spring/blob/main/W11_0517.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1>Week 11.\n",
        "<br>\n",
        "Natural Language Processing 2</h1>"
      ],
      "metadata": {
        "id": "zedMIjXdKJ-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wt47I81jJ8hi"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ErikaJacobs/Harry-Potter-Text-Mining.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word count"
      ],
      "metadata": {
        "id": "Sb2hv8N4Y8IA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Load data</h3>"
      ],
      "metadata": {
        "id": "RorVmdSlbmsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "text_paths = glob.glob(\"Harry-Potter-Text-Mining/Book Text/*.txt\")"
      ],
      "metadata": {
        "id": "EEn4KIrBK2Z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame()\n",
        "\n",
        "for text_path in sorted(text_paths):\n",
        "  text_df = pd.read_csv(text_path, sep=\"@\")\n",
        "  df = pd.concat([df, text_df])\n",
        "\n",
        "df = df.reset_index().drop(\"index\", axis=1)"
      ],
      "metadata": {
        "id": "VFR08csaLZiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Tokenize</h3>"
      ],
      "metadata": {
        "id": "CmCA4-_tZHjz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "retokenize = RegexpTokenizer(\"[\\w]+\")"
      ],
      "metadata": {
        "id": "mEz-icEgMQRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Tokens\"] = df[\"Text\"].str.lower().apply(retokenize.tokenize)"
      ],
      "metadata": {
        "id": "qxzCEsdeQmBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Number of words in each book</h3>"
      ],
      "metadata": {
        "id": "bFcuzBjIZNWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"WordCount\"] = df[\"Tokens\"].str.len()"
      ],
      "metadata": {
        "id": "cNJ3VNmKRHlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "titles=[\"Philosopher's Stone\", \"Chamber of Secrets\", \"Prisoner of Azkaban\", \"Goblet of Fire\", \"Order of the Phoenix\", \"Half Blood Prince\", \"Deathly Hallows\"]\n",
        "\n",
        "total_result = df.groupby(\"Book\")[\"WordCount\"].sum().reset_index()\n",
        "total_result.plot(x=\"Book\", y=\"WordCount\", kind=\"bar\", figsize=(15, 10), color=['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'], legend=False)\n",
        "\n",
        "plt.xticks(range(7), titles)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JyaURY4ARQDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Average number of words per chapter of each book</h3>"
      ],
      "metadata": {
        "id": "z68R80giaPtp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_result = df.groupby(\"Book\")[\"WordCount\"].mean().reset_index()\n",
        "mean_result.plot(x=\"Book\", y=\"WordCount\", kind=\"bar\", figsize=(15, 10), color=['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'], legend=False)\n",
        "\n",
        "plt.xticks(range(7), titles)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XFdPyZC8Z8k3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Shortest chapter of each book</h3>"
      ],
      "metadata": {
        "id": "9c4aRHiWZlvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_result = df.groupby(\"Book\")[\"WordCount\"].min().reset_index()\n",
        "min_result.plot(x=\"Book\", y=\"WordCount\", kind=\"bar\", figsize=(15, 10), color=['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'], legend=False)\n",
        "\n",
        "plt.xticks(range(7), titles)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fYcUNlyqW1iy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Longest chapter of each book</h3>"
      ],
      "metadata": {
        "id": "zqaysPJOalRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_result = df.groupby(\"Book\")[\"WordCount\"].max().reset_index()\n",
        "max_result.plot(x=\"Book\", y=\"WordCount\", kind=\"bar\", figsize=(15, 10), color=['#DC8458', '#950702', '#8E067D', '#2E8C44', '#395196', '#60A619','#ECA10A'], legend=False)\n",
        "\n",
        "plt.xticks(range(7), titles)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nSuPWICdWc1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Shortest chapter of each book (DataFrame)</h3>"
      ],
      "metadata": {
        "id": "zjGHyrCoao26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_book = min_result[\"Book\"]\n",
        "min_word_count = min_result[\"WordCount\"]\n",
        "\n",
        "min_df = pd.DataFrame()\n",
        "\n",
        "for b, wc in zip(min_book, min_word_count):\n",
        "  min_df = pd.concat([min_df, df[(df[\"WordCount\"] == wc) & (df[\"Book\"] == b)]])"
      ],
      "metadata": {
        "id": "dZYfnh5DTIiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_df"
      ],
      "metadata": {
        "id": "4XEug3EAY3I-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Longest chapter of each book (DataFrame)</h3>"
      ],
      "metadata": {
        "id": "v487eb2Aa1da"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_book = max_result[\"Book\"]\n",
        "max_word_count = max_result[\"WordCount\"]\n",
        "\n",
        "max_df = pd.DataFrame()\n",
        "\n",
        "for b, wc in zip(max_book, max_word_count):\n",
        "  max_df = pd.concat([max_df, df[(df[\"WordCount\"] == wc) & (df[\"Book\"] == b)]])"
      ],
      "metadata": {
        "id": "DZvyMBIvY-Tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_df"
      ],
      "metadata": {
        "id": "yegyYRuQZC7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentiment analysis"
      ],
      "metadata": {
        "id": "sAJ1crtHbBv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Load data</h3>"
      ],
      "metadata": {
        "id": "mq_XF2RAbO99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = pd.read_csv(\"sent_df.csv\", sep=\"@\")"
      ],
      "metadata": {
        "id": "X14lgbmjiB5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')\n",
        "analyzer = nltk.sentiment.vader.SentimentIntensityAnalyzer()"
      ],
      "metadata": {
        "id": "C0YqnmbnNH9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Sentiment score</h3>"
      ],
      "metadata": {
        "id": "OLH3lICVbSau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df[\"Score\"] = sent_df[\"Sentence\"].apply(analyzer.polarity_scores)"
      ],
      "metadata": {
        "id": "Y6_kNLviNS5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df[\"TotalScore\"] = sent_df[\"Score\"].apply(lambda x: x[\"compound\"])\n",
        "sent_df[\"PosScore\"] = sent_df[\"Score\"].apply(lambda x: x[\"pos\"])\n",
        "sent_df[\"NeuScore\"] = sent_df[\"Score\"].apply(lambda x: x[\"neu\"])\n",
        "sent_df[\"NegScore\"] = sent_df[\"Score\"].apply(lambda x: x[\"neg\"])"
      ],
      "metadata": {
        "id": "8V0bBlWNUFzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df = sent_df.drop([\"Score\"], axis=1)  # Drop the original score\n",
        "\n",
        "sent_df[\"PosFlag\"] = sent_df[\"TotalScore\"].apply(lambda x: x >= 0.05)\n",
        "sent_df[\"NeuFlag\"] = sent_df[\"TotalScore\"].apply(lambda x: x > -0.05 and x < 0.05)\n",
        "sent_df[\"NegFlag\"] = sent_df[\"TotalScore\"].apply(lambda x: x <= -0.05)"
      ],
      "metadata": {
        "id": "grDd9PGEVcAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.groupby(\"Book\")[\"TotalScore\"].mean()"
      ],
      "metadata": {
        "id": "88oTBgIOV-Vu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"There are {sent_df['PosFlag'].sum()} positive sentences.\")\n",
        "print(f\"There are {sent_df['NegFlag'].sum()} negative sentences.\")\n",
        "print(f\"There are {sent_df['NeuFlag'].sum()} neutral sentences.\")"
      ],
      "metadata": {
        "id": "BDaKTNFmWf4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h3>Visualization</h3>"
      ],
      "metadata": {
        "id": "-euG7sobbWaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent_df.groupby([\"Chapter\", \"Book\"])[\"TotalScore\"].mean().unstack().plot(subplots=True, figsize=(15, 10), ylim=(-0.3, 0.3))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gGhyeoDEXgQF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}